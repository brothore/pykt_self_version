{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "548d8f15-bbbd-4f06-99a0-e8fefe2493b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在读取CSV文件...\n",
      "共读取到 39807 条学生记录\n",
      "正在分类学生...\n",
      "分类结果统计:\n",
      "  均衡型: 38408 人\n",
      "  偏科型: 1399 人\n",
      "偏科型学生数据已保存到: /root/autodl-tmp/pykt_self_version/data/assist2009//sp.csv (1399 人)\n",
      "均衡型学生数据已保存到: /root/autodl-tmp/pykt_self_version/data/assist2009//ba.csv (38408 人)\n",
      "处理完成！\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Tuple, List\n",
    "\n",
    "def classify_student_type(row: pd.Series) -> str:\n",
    "    \"\"\"\n",
    "    分类函数：判断学生是偏科型还是均衡型\n",
    "    \n",
    "    偏科型判定标准：\n",
    "    1. 显著性过滤：强/弱知识点需满足最低题量阈值（≥5题）\n",
    "       - 强项知识点：pk > 0.8 且 Nk ≥ 5\n",
    "       - 弱项知识点：pk < 0.4 且 Nk ≥ 5\n",
    "    2. 效应量评估：计算强项与弱项的Cohen's d值\n",
    "       - 仅当 |d| > 0.8 时判定为显著偏科\n",
    "    \n",
    "    参数:\n",
    "        row: 包含学生数据的pandas Series\n",
    "    \n",
    "    返回:\n",
    "        str: \"偏科型\" 或 \"均衡型\"\n",
    "    \"\"\"\n",
    "    # 解析数据并排除填充值-1\n",
    "    all_concepts = [int(x) for x in row['concepts'].split(',')]\n",
    "    all_responses = [int(x) for x in row['responses'].split(',')]\n",
    "    \n",
    "    # 过滤掉填充值-1\n",
    "    valid_data = [(concept, response) for concept, response in zip(all_concepts, all_responses) \n",
    "                  if concept != -1 and response != -1]\n",
    "    \n",
    "    if not valid_data:\n",
    "        return \"均衡型\"  # 如果没有有效数据，默认为均衡型\n",
    "    \n",
    "    concepts, responses = zip(*valid_data)\n",
    "    concepts = list(concepts)\n",
    "    responses = list(responses)\n",
    "    \n",
    "    # 计算每个概念的正确率和题量\n",
    "    concept_accuracy = calculate_concept_accuracy(concepts, responses)\n",
    "    \n",
    "    # 显著性过滤：识别强项和弱项知识点\n",
    "    strong_concepts = {}  # 强项知识点\n",
    "    weak_concepts = {}    # 弱项知识点\n",
    "    \n",
    "    for concept, accuracy in concept_accuracy.items():\n",
    "        # 计算该概念的题量\n",
    "        concept_count = concepts.count(concept)\n",
    "        \n",
    "        # 强项知识点：正确率 > 0.8 且题量 ≥ 5\n",
    "        if accuracy > 0.8 and concept_count >= 5:\n",
    "            strong_concepts[concept] = {\n",
    "                'accuracy': accuracy,\n",
    "                'count': concept_count\n",
    "            }\n",
    "        \n",
    "        # 弱项知识点：正确率 < 0.4 且题量 ≥ 5\n",
    "        elif accuracy < 0.4 and concept_count >= 5:\n",
    "            weak_concepts[concept] = {\n",
    "                'accuracy': accuracy,\n",
    "                'count': concept_count\n",
    "            }\n",
    "    \n",
    "    # 如果没有同时存在强项和弱项，则为均衡型\n",
    "    if not strong_concepts or not weak_concepts:\n",
    "        return \"均衡型\"\n",
    "    \n",
    "    # 计算Cohen's d值\n",
    "    cohens_d = calculate_cohens_d(strong_concepts, weak_concepts)\n",
    "    \n",
    "    # 判定标准：|d| > 0.8 为显著偏科\n",
    "    if abs(cohens_d) > 0.8:\n",
    "        return \"偏科型\"\n",
    "    else:\n",
    "        return \"均衡型\"\n",
    "\n",
    "def parse_student_data(row: pd.Series) -> dict:\n",
    "    \"\"\"\n",
    "    解析学生数据，将字符串格式的列表转换为实际的列表，并排除填充值-1\n",
    "    \n",
    "    参数:\n",
    "        row: 原始数据行\n",
    "    \n",
    "    返回:\n",
    "        dict: 解析后的数据字典\n",
    "    \"\"\"\n",
    "    # 解析所有数据\n",
    "    all_questions = [int(x) for x in row['questions'].split(',')]\n",
    "    all_concepts = [int(x) for x in row['concepts'].split(',')]\n",
    "    all_responses = [int(x) for x in row['responses'].split(',')]\n",
    "    all_is_repeat = [int(x) for x in row['is_repeat'].split(',')]\n",
    "    \n",
    "    # 过滤掉填充值-1（以concepts和responses为准，因为这两个是分类的关键字段）\n",
    "    valid_indices = [i for i, (concept, response) in enumerate(zip(all_concepts, all_responses)) \n",
    "                     if concept != -1 and response != -1]\n",
    "    \n",
    "    return {\n",
    "        'fold': row['fold'],\n",
    "        'uid': row['uid'],\n",
    "        'questions': [all_questions[i] for i in valid_indices if i < len(all_questions)],\n",
    "        'concepts': [all_concepts[i] for i in valid_indices],\n",
    "        'responses': [all_responses[i] for i in valid_indices],\n",
    "        'is_repeat': [all_is_repeat[i] for i in valid_indices if i < len(all_is_repeat)]\n",
    "    }\n",
    "\n",
    "def calculate_cohens_d(strong_concepts: dict, weak_concepts: dict) -> float:\n",
    "    \"\"\"\n",
    "    计算强项与弱项知识点之间的Cohen's d效应量\n",
    "    \n",
    "    公式：d = (p_strong - p_weak) / sqrt(((N_strong-1)*s_strong^2 + (N_weak-1)*s_weak^2) / (N_strong + N_weak - 2))\n",
    "    \n",
    "    参数:\n",
    "        strong_concepts: 强项知识点字典 {concept_id: {'accuracy': float, 'count': int}}\n",
    "        weak_concepts: 弱项知识点字典 {concept_id: {'accuracy': float, 'count': int}}\n",
    "    \n",
    "    返回:\n",
    "        float: Cohen's d值\n",
    "    \"\"\"\n",
    "    # 提取强项数据\n",
    "    strong_accuracies = [data['accuracy'] for data in strong_concepts.values()]\n",
    "    strong_counts = [data['count'] for data in strong_concepts.values()]\n",
    "    \n",
    "    # 提取弱项数据\n",
    "    weak_accuracies = [data['accuracy'] for data in weak_concepts.values()]\n",
    "    weak_counts = [data['count'] for data in weak_concepts.values()]\n",
    "    \n",
    "    # 计算加权平均正确率\n",
    "    strong_total_questions = sum(strong_counts)\n",
    "    weak_total_questions = sum(weak_counts)\n",
    "    \n",
    "    p_strong = sum(acc * count for acc, count in zip(strong_accuracies, strong_counts)) / strong_total_questions\n",
    "    p_weak = sum(acc * count for acc, count in zip(weak_accuracies, weak_counts)) / weak_total_questions\n",
    "    \n",
    "    # 计算加权方差\n",
    "    strong_variance = sum(count * (acc - p_strong)**2 for acc, count in zip(strong_accuracies, strong_counts)) / strong_total_questions\n",
    "    weak_variance = sum(count * (acc - p_weak)**2 for acc, count in zip(weak_accuracies, weak_counts)) / weak_total_questions\n",
    "    \n",
    "    # 计算合并标准差\n",
    "    n_strong = len(strong_concepts)\n",
    "    n_weak = len(weak_concepts)\n",
    "    \n",
    "    if n_strong + n_weak <= 2:\n",
    "        return 0.0\n",
    "    \n",
    "    pooled_variance = ((n_strong - 1) * strong_variance + (n_weak - 1) * weak_variance) / (n_strong + n_weak - 2)\n",
    "    \n",
    "    if pooled_variance == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    pooled_std = np.sqrt(pooled_variance)\n",
    "    \n",
    "    # 计算Cohen's d\n",
    "    cohens_d = (p_strong - p_weak)\n",
    "    # cohens_d = (p_strong - p_weak) / pooled_std\n",
    "    \n",
    "    return cohens_d\n",
    "    \"\"\"\n",
    "    计算每个概念的正确率\n",
    "    \n",
    "    参数:\n",
    "        concepts: 概念ID列表\n",
    "        responses: 对应的回答结果列表\n",
    "    \n",
    "    返回:\n",
    "        dict: 概念ID到正确率的映射\n",
    "    \"\"\"\n",
    "    concept_stats = {}\n",
    "    \n",
    "    for concept, response in zip(concepts, responses):\n",
    "        if concept not in concept_stats:\n",
    "            concept_stats[concept] = {'correct': 0, 'total': 0}\n",
    "        \n",
    "        concept_stats[concept]['total'] += 1\n",
    "        if response == 1:\n",
    "            concept_stats[concept]['correct'] += 1\n",
    "    \n",
    "    # 计算正确率\n",
    "    concept_accuracy = {}\n",
    "    for concept, stats in concept_stats.items():\n",
    "        concept_accuracy[concept] = stats['correct'] / stats['total']\n",
    "    \n",
    "    return concept_accuracy\n",
    "\n",
    "def main(input_csv_path: str, output_dir: str = './'):\n",
    "    \"\"\"\n",
    "    主函数：读取CSV文件，分类学生，保存结果\n",
    "    \n",
    "    参数:\n",
    "        input_csv_path: 输入CSV文件路径\n",
    "        output_dir: 输出目录路径\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. 读取CSV文件\n",
    "    print(\"正在读取CSV文件...\")\n",
    "    df = pd.read_csv(input_csv_path)\n",
    "    print(f\"共读取到 {len(df)} 条学生记录\")\n",
    "    \n",
    "    # 2. 分类学生\n",
    "    print(\"正在分类学生...\")\n",
    "    df['student_type'] = df.apply(classify_student_type, axis=1)\n",
    "    \n",
    "    # 3. 统计分类结果\n",
    "    type_counts = df['student_type'].value_counts()\n",
    "    print(\"分类结果统计:\")\n",
    "    for student_type, count in type_counts.items():\n",
    "        print(f\"  {student_type}: {count} 人\")\n",
    "    \n",
    "    # 4. 分别保存两类学生的数据\n",
    "    specialized_students = df[df['student_type'] == '偏科型'].copy()\n",
    "    balanced_students = df[df['student_type'] == '均衡型'].copy()\n",
    "    \n",
    "    # 移除分类标签列（如果不需要保存的话）\n",
    "    specialized_students = specialized_students.drop('student_type', axis=1)\n",
    "    balanced_students = balanced_students.drop('student_type', axis=1)\n",
    "    \n",
    "    # 5. 保存为新的CSV文件\n",
    "    specialized_output_path = f\"{output_dir}/pk.csv\"\n",
    "    balanced_output_path = f\"{output_dir}/ph.csv\"\n",
    "    \n",
    "    specialized_students.to_csv(specialized_output_path, index=False)\n",
    "    balanced_students.to_csv(balanced_output_path, index=False)\n",
    "    \n",
    "    print(f\"偏科型学生数据已保存到: {specialized_output_path} ({len(specialized_students)} 人)\")\n",
    "    print(f\"均衡型学生数据已保存到: {balanced_output_path} ({len(balanced_students)} 人)\")\n",
    "    \n",
    "    return specialized_students, balanced_students\n",
    "def calculate_concept_accuracy(concepts: List[int], responses: List[int]) -> dict:\n",
    "    \"\"\"\n",
    "    计算每个概念的正确率\n",
    "    \n",
    "    参数:\n",
    "        concepts: 概念ID列表\n",
    "        responses: 对应的回答结果列表\n",
    "    \n",
    "    返回:\n",
    "        dict: 概念ID到正确率的映射\n",
    "    \"\"\"\n",
    "    concept_stats = {}\n",
    "    \n",
    "    for concept, response in zip(concepts, responses):\n",
    "        if concept not in concept_stats:\n",
    "            concept_stats[concept] = {'correct': 0, 'total': 0}\n",
    "        \n",
    "        concept_stats[concept]['total'] += 1\n",
    "        if response == 1:\n",
    "            concept_stats[concept]['correct'] += 1\n",
    "    \n",
    "    # 计算正确率\n",
    "    concept_accuracy = {}\n",
    "    for concept, stats in concept_stats.items():\n",
    "        concept_accuracy[concept] = stats['correct'] / stats['total']\n",
    "    \n",
    "    return concept_accuracy\n",
    "# 使用示例\n",
    "if __name__ == \"__main__\":\n",
    "    # 指定输入文件路径\n",
    "    input_file = \"/root/autodl-tmp/pykt_self_version/data/assist2009/old_test_question_window_sequences.csv\"  # 替换为你的实际文件路径\n",
    "    output_directory = \"/root/autodl-tmp/pykt_self_version/data/assist2009/\"  # 输出目录\n",
    "    \n",
    "    # 执行分类和保存\n",
    "    specialized_df, balanced_df = main(input_file, output_directory)\n",
    "    \n",
    "    print(\"处理完成！\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d4c6ab4-89ac-4939-a345-74e12dcf2333",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/root/autodl-tmp/pykt_self_version/examples/wandb_predict.py\", line 14, in <module>\n",
      "    with open(\"../configs/wandb.json\") as fin:\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '../configs/wandb.json'\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196836a1-2d51-44be-8d58-e18f6966f02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python wandb_predict.py --save_dir \"/root/autodl-tmp/pykt_self_version/examples/saved_model/assist2009_dkt_qid_saved_model_3407_0_0.5_256_0.001_0_1_0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6fd67a9-3fc6-4978-b3fd-22c63d0913fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在读取CSV文件...\n",
      "共读取到 39807 条记录\n",
      "共有 776 个不同的学生\n",
      "\n",
      "正在计算学生知识点掌握差异度...\n",
      "\n",
      "分类结果统计:\n",
      "总记录数: 39807\n",
      "总学生数: 776\n",
      "\n",
      "  偏科型:\n",
      "    记录数: 14880 (37.4%)\n",
      "    学生数: 24 (3.1%)\n",
      "    差值范围: 0.902 - 1.000\n",
      "    平均差值: 0.968\n",
      "\n",
      "  普通型:\n",
      "    记录数: 11674 (29.3%)\n",
      "    学生数: 121 (15.6%)\n",
      "    差值范围: 0.522 - 0.893\n",
      "    平均差值: 0.696\n",
      "\n",
      "  均衡型:\n",
      "    记录数: 13253 (33.3%)\n",
      "    学生数: 631 (81.3%)\n",
      "    差值范围: 0.000 - 0.522\n",
      "    平均差值: 0.108\n",
      "\n",
      "数据已保存:\n",
      "偏科型学生数据: /root/autodl-tmp/pykt_self_version/data/assist2009//pk.csv (14880 条记录, 24 个学生)\n",
      "普通型学生数据: /root/autodl-tmp/pykt_self_version/data/assist2009//pt.csv (11674 条记录, 121 个学生)\n",
      "均衡型学生数据: /root/autodl-tmp/pykt_self_version/data/assist2009//ph.csv (13253 条记录, 631 个学生)\n",
      "\n",
      "示例分析:\n",
      "差值最大的学生: UID=868, 差值=1.000, 类型=偏科型\n",
      "差值最小的学生: UID=1912, 差值=0.000, 类型=均衡型\n",
      "\n",
      "差值分布统计:\n",
      "最小值: 0.000\n",
      "25%分位数: 0.000\n",
      "中位数: 0.000\n",
      "75%分位数: 0.431\n",
      "最大值: 1.000\n",
      "标准差: 0.294\n",
      "\n",
      "处理完成！\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Tuple, List, Dict\n",
    "\n",
    "def calculate_student_variance(uid_data: pd.DataFrame) -> float:\n",
    "    \"\"\"\n",
    "    计算单个学生的知识点正确率差值\n",
    "    使用前20%高正确率知识点和后20%低正确率知识点的平均正确率差值\n",
    "    \n",
    "    参数:\n",
    "        uid_data: 包含同一个uid所有记录的DataFrame\n",
    "    \n",
    "    返回:\n",
    "        float: 高正确率知识点平均值 - 低正确率知识点平均值，如果没有符合条件的知识点返回0\n",
    "    \"\"\"\n",
    "    # 合并所有行的数据\n",
    "    all_concepts = []\n",
    "    all_responses = []\n",
    "    \n",
    "    for _, row in uid_data.iterrows():\n",
    "        concepts = [int(x) for x in row['concepts'].split(',')]\n",
    "        responses = [int(x) for x in row['responses'].split(',')]\n",
    "        \n",
    "        # 过滤掉填充值-1\n",
    "        valid_pairs = [(c, r) for c, r in zip(concepts, responses) if c != -1 and r != -1]\n",
    "        if valid_pairs:\n",
    "            valid_concepts, valid_responses = zip(*valid_pairs)\n",
    "            all_concepts.extend(valid_concepts)\n",
    "            all_responses.extend(valid_responses)\n",
    "    \n",
    "    if not all_concepts:\n",
    "        return 0.0\n",
    "    \n",
    "    # 统计每个概念的正确率和题量\n",
    "    concept_stats = {}\n",
    "    for concept, response in zip(all_concepts, all_responses):\n",
    "        if concept not in concept_stats:\n",
    "            concept_stats[concept] = {'correct': 0, 'total': 0}\n",
    "        \n",
    "        concept_stats[concept]['total'] += 1\n",
    "        if response == 1:\n",
    "            concept_stats[concept]['correct'] += 1\n",
    "    \n",
    "    # 只考虑做题数量>5的知识点\n",
    "    valid_concept_accuracies = []\n",
    "    for concept, stats in concept_stats.items():\n",
    "        if stats['total'] > 5:  # 题量大于5\n",
    "            accuracy = stats['correct'] / stats['total']\n",
    "            valid_concept_accuracies.append((concept, accuracy))\n",
    "    \n",
    "    # 如果没有符合条件的知识点，返回0\n",
    "    if not valid_concept_accuracies:\n",
    "        return 0.0\n",
    "    \n",
    "    # 如果只有一个知识点，差值为0\n",
    "    if len(valid_concept_accuracies) == 1:\n",
    "        return 0.0\n",
    "    \n",
    "    # 按正确率排序\n",
    "    valid_concept_accuracies.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # 计算前20%和后20%的数量\n",
    "    n_concepts = len(valid_concept_accuracies)\n",
    "    n_top = max(1, int(n_concepts * 0.2))  # 至少取1个\n",
    "    n_bottom = max(1, int(n_concepts * 0.2))  # 至少取1个\n",
    "    \n",
    "    # 获取前20%的知识点正确率\n",
    "    top_accuracies = [acc for _, acc in valid_concept_accuracies[:n_top]]\n",
    "    # 获取后20%的知识点正确率\n",
    "    bottom_accuracies = [acc for _, acc in valid_concept_accuracies[-n_bottom:]]\n",
    "    \n",
    "    # 计算平均值\n",
    "    avg_top = sum(top_accuracies) / len(top_accuracies)\n",
    "    avg_bottom = sum(bottom_accuracies) / len(bottom_accuracies)\n",
    "    \n",
    "    # 返回差值\n",
    "    variance = avg_top - avg_bottom\n",
    "    \n",
    "    return variance\n",
    "\n",
    "def classify_students_by_variance(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    根据正确率差值将学生分为三类，按记录数均衡分配\n",
    "    前33%记录为偏科型，后33%记录为均衡型，中间为普通型\n",
    "    \n",
    "    参数:\n",
    "        df: 原始数据DataFrame\n",
    "    \n",
    "    返回:\n",
    "        pd.DataFrame: 添加了分类标签的DataFrame\n",
    "    \"\"\"\n",
    "    # 按uid分组计算每个学生的差值和记录数\n",
    "    student_info = {}\n",
    "    \n",
    "    for uid, group in df.groupby('uid'):\n",
    "        variance = calculate_student_variance(group)\n",
    "        record_count = len(group)\n",
    "        student_info[uid] = {\n",
    "            'variance': variance,\n",
    "            'record_count': record_count\n",
    "        }\n",
    "    \n",
    "    # 创建学生级别的DataFrame\n",
    "    student_df = pd.DataFrame.from_dict(student_info, orient='index')\n",
    "    student_df.reset_index(inplace=True)\n",
    "    student_df.columns = ['uid', 'variance', 'record_count']\n",
    "    \n",
    "    # 按差值降序排序（差值大的在前）\n",
    "    student_df = student_df.sort_values('variance', ascending=False)\n",
    "    \n",
    "    # 计算总记录数\n",
    "    total_records = student_df['record_count'].sum()\n",
    "    target_per_group = total_records / 3\n",
    "    \n",
    "    # 贪心算法分配学生到三类\n",
    "    student_df['student_type'] = ''\n",
    "    cumulative_records = 0\n",
    "    \n",
    "    # 第一阶段：分配偏科型（前33%记录）\n",
    "    for idx, row in student_df.iterrows():\n",
    "        if cumulative_records < target_per_group:\n",
    "            student_df.at[idx, 'student_type'] = '偏科型'\n",
    "            cumulative_records += row['record_count']\n",
    "        else:\n",
    "            # 检查是否加入这个学生会更接近目标\n",
    "            if abs(cumulative_records - target_per_group) > abs(cumulative_records + row['record_count'] - target_per_group):\n",
    "                student_df.at[idx, 'student_type'] = '偏科型'\n",
    "                cumulative_records += row['record_count']\n",
    "            else:\n",
    "                break\n",
    "    \n",
    "    # 第二阶段：分配普通型（中间33%记录）\n",
    "    for idx, row in student_df.iterrows():\n",
    "        if student_df.at[idx, 'student_type'] == '':\n",
    "            if cumulative_records < 2 * target_per_group:\n",
    "                student_df.at[idx, 'student_type'] = '普通型'\n",
    "                cumulative_records += row['record_count']\n",
    "            else:\n",
    "                # 检查是否加入这个学生会更接近目标\n",
    "                if abs(cumulative_records - 2 * target_per_group) > abs(cumulative_records + row['record_count'] - 2 * target_per_group):\n",
    "                    student_df.at[idx, 'student_type'] = '普通型'\n",
    "                    cumulative_records += row['record_count']\n",
    "                else:\n",
    "                    break\n",
    "    \n",
    "    # 第三阶段：剩余的都是均衡型\n",
    "    student_df.loc[student_df['student_type'] == '', 'student_type'] = '均衡型'\n",
    "    \n",
    "    # 将分类结果合并回原始DataFrame\n",
    "    type_mapping = dict(zip(student_df['uid'], student_df['student_type']))\n",
    "    variance_mapping = dict(zip(student_df['uid'], student_df['variance']))\n",
    "    \n",
    "    df['variance'] = df['uid'].map(variance_mapping)\n",
    "    df['student_type'] = df['uid'].map(type_mapping)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def main(input_csv_path: str, output_dir: str = './'):\n",
    "    \"\"\"\n",
    "    主函数：读取CSV文件，分类学生，保存结果\n",
    "    \n",
    "    参数:\n",
    "        input_csv_path: 输入CSV文件路径\n",
    "        output_dir: 输出目录路径\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. 读取CSV文件\n",
    "    print(\"正在读取CSV文件...\")\n",
    "    df = pd.read_csv(input_csv_path)\n",
    "    print(f\"共读取到 {len(df)} 条记录\")\n",
    "    print(f\"共有 {df['uid'].nunique()} 个不同的学生\")\n",
    "    \n",
    "    # 2. 按差值分类学生\n",
    "    print(\"\\n正在计算学生知识点掌握差异度...\")\n",
    "    df_classified = classify_students_by_variance(df)\n",
    "    \n",
    "    # 3. 统计分类结果\n",
    "    student_summary = df_classified[['uid', 'student_type', 'variance']].drop_duplicates()\n",
    "    \n",
    "    # 统计记录数\n",
    "    record_counts = df_classified['student_type'].value_counts()\n",
    "    total_records = len(df_classified)\n",
    "    \n",
    "    # 统计学生数\n",
    "    student_counts = student_summary['student_type'].value_counts()\n",
    "    \n",
    "    print(\"\\n分类结果统计:\")\n",
    "    print(f\"总记录数: {total_records}\")\n",
    "    print(f\"总学生数: {len(student_summary)}\")\n",
    "    \n",
    "    for student_type in ['偏科型', '普通型', '均衡型']:\n",
    "        if student_type in record_counts.index:\n",
    "            record_count = record_counts[student_type]\n",
    "            record_percentage = (record_count / total_records) * 100\n",
    "            \n",
    "            student_count = student_counts[student_type]\n",
    "            student_percentage = (student_count / len(student_summary)) * 100\n",
    "            \n",
    "            print(f\"\\n  {student_type}:\")\n",
    "            print(f\"    记录数: {record_count} ({record_percentage:.1f}%)\")\n",
    "            print(f\"    学生数: {student_count} ({student_percentage:.1f}%)\")\n",
    "            \n",
    "            # 显示该类型学生的差值范围\n",
    "            type_variances = student_summary[student_summary['student_type'] == student_type]['variance']\n",
    "            if len(type_variances) > 0:\n",
    "                print(f\"    差值范围: {type_variances.min():.3f} - {type_variances.max():.3f}\")\n",
    "                print(f\"    平均差值: {type_variances.mean():.3f}\")\n",
    "    \n",
    "    # 4. 分别保存三类学生的数据\n",
    "    specialized_students = df_classified[df_classified['student_type'] == '偏科型'].copy()\n",
    "    normal_students = df_classified[df_classified['student_type'] == '普通型'].copy()\n",
    "    balanced_students = df_classified[df_classified['student_type'] == '均衡型'].copy()\n",
    "    \n",
    "    # 移除临时列\n",
    "    for df_subset in [specialized_students, normal_students, balanced_students]:\n",
    "        df_subset.drop(['student_type', 'variance'], axis=1, inplace=True)\n",
    "    \n",
    "    # 5. 保存为新的CSV文件\n",
    "    specialized_output_path = f\"{output_dir}/pk.csv\"\n",
    "    normal_output_path = f\"{output_dir}/pt.csv\"\n",
    "    balanced_output_path = f\"{output_dir}/ph.csv\"\n",
    "    \n",
    "    specialized_students.to_csv(specialized_output_path, index=False)\n",
    "    normal_students.to_csv(normal_output_path, index=False)\n",
    "    balanced_students.to_csv(balanced_output_path, index=False)\n",
    "    \n",
    "    print(f\"\\n数据已保存:\")\n",
    "    print(f\"偏科型学生数据: {specialized_output_path} ({len(specialized_students)} 条记录, {specialized_students['uid'].nunique()} 个学生)\")\n",
    "    print(f\"普通型学生数据: {normal_output_path} ({len(normal_students)} 条记录, {normal_students['uid'].nunique()} 个学生)\")\n",
    "    print(f\"均衡型学生数据: {balanced_output_path} ({len(balanced_students)} 条记录, {balanced_students['uid'].nunique()} 个学生)\")\n",
    "    \n",
    "    # 6. 显示一些示例\n",
    "    print(\"\\n示例分析:\")\n",
    "    # 显示差值最大的学生\n",
    "    top_student = student_summary.nlargest(1, 'variance').iloc[0]\n",
    "    print(f\"差值最大的学生: UID={top_student['uid']}, 差值={top_student['variance']:.3f}, 类型={top_student['student_type']}\")\n",
    "    \n",
    "    # 显示差值最小的学生\n",
    "    bottom_student = student_summary.nsmallest(1, 'variance').iloc[0]\n",
    "    print(f\"差值最小的学生: UID={bottom_student['uid']}, 差值={bottom_student['variance']:.3f}, 类型={bottom_student['student_type']}\")\n",
    "    \n",
    "    # 显示差值分布信息\n",
    "    print(f\"\\n差值分布统计:\")\n",
    "    print(f\"最小值: {student_summary['variance'].min():.3f}\")\n",
    "    print(f\"25%分位数: {student_summary['variance'].quantile(0.25):.3f}\")\n",
    "    print(f\"中位数: {student_summary['variance'].median():.3f}\")\n",
    "    print(f\"75%分位数: {student_summary['variance'].quantile(0.75):.3f}\")\n",
    "    print(f\"最大值: {student_summary['variance'].max():.3f}\")\n",
    "    print(f\"标准差: {student_summary['variance'].std():.3f}\")\n",
    "    \n",
    "    return specialized_students, normal_students, balanced_students\n",
    "\n",
    "# 使用示例\n",
    "if __name__ == \"__main__\":\n",
    "    # 指定输入文件路径\n",
    "    input_file = \"/root/autodl-tmp/pykt_self_version/data/assist2009/old_test_question_window_sequences.csv\"\n",
    "    output_directory = \"/root/autodl-tmp/pykt_self_version/data/assist2009/\"\n",
    "    \n",
    "    # 执行分类和保存\n",
    "    specialized_df, normal_df, balanced_df = main(input_file, output_directory)\n",
    "    \n",
    "    print(\"\\n处理完成！\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef36d8b-5ffd-4147-b4d4-6b006050c8df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
