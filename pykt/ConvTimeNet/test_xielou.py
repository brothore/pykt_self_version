from pykt.ConvTimeNet.Patch_layers import *
from pykt.ConvTimeNet.RevIN import RevIN
from pykt.ConvTimeNet.ConvTimeNet_backbone import ConvTimeNet_backbone
import argparse


device = torch.device("cuda" if torch.cuda.is_available() else "cpu")  # åŠ¨æ€è·å–è®¾å¤‡

parser = argparse.ArgumentParser()
parser.add_argument("--dataset_name", type=str, default="assist2009")
parser.add_argument("--model_name", type=str, default="CTNKT")
parser.add_argument("--emb_type", type=str, default="qid")
parser.add_argument("--save_dir", type=str, default="saved_model")
# parser.add_argument("--learning_rate", type=float, default=2e-3)
parser.add_argument("--seed", type=int, default=2021)
parser.add_argument("--fold", type=int, default=0)
parser.add_argument("--emb_size", type=int, default=256)
# parser.add_argument("--input_size", type=int, default=1)
# parser.add_argument("--output_size", type=int, default=1)

#ConvTimeNet å‚æ•°
parser.add_argument('--model', type=str, required=False, default='DePatchConv',
                help='model name, options: [Autoformer, Informer, Transformer]')
# å˜é‡ï¼Œå•ä¸ªè¿˜æ˜¯å¤šä¸ª
parser.add_argument('--features', type=str, default='M',
                help='forecasting task, options:[M, S, MS]; M:multivariate predict multivariate, S:univariate predict univariate, MS:multivariate predict univariate')
parser.add_argument('--target', type=str, default='OT', help='target feature in S or MS task')
parser.add_argument('--freq', type=str, default='h',
                help='freq for time features encoding, options:[s:secondly, t:minutely, h:hourly, d:daily, b:business days, w:weekly, m:monthly], you can also use more detailed freq like 15min or 3h')
parser.add_argument('--seq_len', type=int, default=215, help='input sequence length')
parser.add_argument('--label_len', type=int, default=48, help='start token length')
parser.add_argument('--pred_len', type=int, default=199, help='prediction sequence length')
# ConvTimeNet
parser.add_argument('--head_dropout', type=float, default=0.0, help='head dropout')
parser.add_argument('--padding_patch', default='end', help='None: None; end: padding on the end')
parser.add_argument('--revin', type=int, default=1, help='RevIN; True 1 False 0')
parser.add_argument('--affine', type=int, default=0, help='RevIN-affine; True 1 False 0')
parser.add_argument('--subtract_last', type=int, default=0, help='0: subtract mean; 1: subtract last')

parser.add_argument('--dw_ks', type=str, default='37,37,43,43,53,53', help="kernel size of the deep-wise. default:9")
parser.add_argument('--re_param', type=int, default=1, help='Reparam the DeepWise Conv when train')
parser.add_argument('--enable_res_param', type=int, default=1, help='Learnable residual')
parser.add_argument('--re_param_kernel', type=int, default=3)

# Patch
parser.add_argument('--patch_ks', type=int, default=32, help="kernel size of the patch window. default:32") #patch_len = configs["patch_ks"]  #patch_count
parser.add_argument('--patch_sd', type=float, default=0.5, \
                    help="stride of the patch window. default: 0.5. if < 1, then sd = patch_sd * patch_ks")


# Other Parameter
parser.add_argument('--enc_in', type=int, default=123, help='encoder input size') 
parser.add_argument('--c_out', type=int, default=123, help='output size')
parser.add_argument('--d_model', type=int, default=64, help='dimension of model')
parser.add_argument('--e_layers', type=int, default=6, help='num of encoder layers')
parser.add_argument('--d_ff', type=int, default=256, help='dimension of fcn')

parser.add_argument('--dropout', type=float, default=0.05, help='dropout')
parser.add_argument('--embed', type=str, default='timeF',
                    help='time features encoding, options:[timeF, fixed, learned]')
parser.add_argument('--activation', type=str, default='gelu', help='activation')
parser.add_argument('--do_predict', action='store_true', help='whether to predict unseen future data')

# optimization
parser.add_argument('--num_workers', type=int, default=10, help='data loader num workers')
parser.add_argument('--itr', type=int, default=2, help='experiments times')
parser.add_argument('--train_epochs', type=int, default=10, help='train epochs')
parser.add_argument('--batch_size', type=int, default=
                    
                    8, help='batch size of train input data')
parser.add_argument('--patience', type=int, default=3, help='early stopping patience')
parser.add_argument('--learning_rate', type=float, default=0.0001, help='optimizer learning rate')
parser.add_argument('--des', type=str, default='test', help='exp description')
parser.add_argument('--loss', type=str, default='mse', help='loss function')
parser.add_argument('--use_amp', action='store_true', help='use automatic mixed precision training', default=False)

# GPU
parser.add_argument('--CTX', type=str, default='0', required=False, help='visuable device ids')
parser.add_argument('--use_gpu', type=bool, default=True, help='use gpu')
parser.add_argument('--gpu', type=int, default=0, help='gpu')
parser.add_argument('--use_multi_gpu', action='store_true', help='use multiple gpus', default=False)
parser.add_argument('--devices', type=str, default='0,1,2,3', help='device ids of multile gpus')
parser.add_argument('--test_flop', action='store_true', default=False, help='See utils/tools for usage')



parser.add_argument("--use_wandb", type=int, default=0)
parser.add_argument("--add_uuid", type=int, default=1)
parser.add_argument("--name", type=str, default="use_caps")
parser.add_argument("--predict_after_train", type=int, default="0")


args = parser.parse_args()
configs = vars(args) 








# æµ‹è¯•ç”¨ä¾‹
# def test_causal_constraint():
#     device = torch.device("cuda" if torch.cuda.is_available() else "cpu")  # åŠ¨æ€è·å–è®¾å¤‡

#     seq_len = 100
#     patch_size = 10
#     stride = 5
#     model = DepatchSampling(in_feats=1, seq_len=seq_len, patch_size=patch_size, stride=stride).to(device)
    
#     # æ„é€ è¾“å…¥æ•°æ®ï¼ˆå‡è®¾å½“å‰æ—¶é—´æ­¥ä¸ºt=50ï¼‰
#     X = torch.randn(1, 1, seq_len).to(device)   # [B=1, C=1, L=100]
    
#     # å‰å‘ä¼ æ’­
#     output = model(X)
    
#     # æ£€æŸ¥è¡¥ä¸çš„æ—¶é—´èŒƒå›´æ˜¯å¦ä¸è¶…è¿‡å½“å‰æ—¶é—´
#     _, _, patch_count, patch_size = output.shape
#     for i in range(patch_count):
#         start = i * stride
#         end = start + patch_size
#         assert end <= seq_len, f"è¡¥ä¸{i}è¶Šç•Œï¼š{end} > {seq_len}"
        
#     print("æµ‹è¯•é€šè¿‡ï¼æ‰€æœ‰è¡¥ä¸æœªæ³„éœ²æœªæ¥ä¿¡æ¯ã€‚")

# test_causal_constraint()
# æµ‹è¯•å·¦ä¾§å¡«å……æ˜¯å¦å¯¼è‡´è¡¥ä¸è¦†ç›–æœªæ¥æ•°æ®
    
num_c = 123
model_name = "CTNKT"

# print(f"configs{configs}")
# load parameters å‚æ•°åˆå§‹åŒ–
c_in = configs["enc_in"]
# c_in = configs["emb_size"]
context_window = configs["seq_len"]
target_window = configs["pred_len"]
emb_size = configs["emb_size"]
# æ¨¡å‹ç»“æ„å‚æ•°
n_layers = configs["e_layers"]
d_model = configs["d_model"]
d_ff = configs["d_ff"]
dropout = configs["dropout"]
head_dropout = configs["head_dropout"]
# åˆ†å—ç›¸å…³å‚æ•°
patch_len = configs["patch_ks"]
# print(f"patch_len{patch_len}")
patch_sd = max(1, int(configs["patch_ks"] * configs["patch_sd"])) if configs["patch_sd"] <= 1 else int(configs["patch_sd"])
stride = patch_sd
print(f"stridesd: {stride}")
padding_patch = configs["padding_patch"]
# å½’ä¸€åŒ–ç›¸å…³å‚æ•°
revin = configs["revin"]
affine = configs["affine"]
subtract_last = configs["subtract_last"]
# æ·±åº¦å·ç§¯å‚æ•°
seq_len = configs["seq_len"]
# print(f"seq_len{seq_len}")
dw_ks = configs["dw_ks"]
emb_type = 256
re_param = configs["re_param"]
re_param_kernel = configs["re_param_kernel"]
enable_res_param = configs["enable_res_param"]
interaction_emb = nn.Embedding(num_c * 2,num_c)
padding_stride = stride  # å¡«å……é‡
norm='batch'
act="gelu" 
head_type = 'flatten'


model = ConvTimeNet_backbone(c_in=c_in, seq_len=seq_len, context_window = context_window,
                            target_window=target_window, patch_len=patch_len, stride=stride, n_layers=n_layers, d_model=d_model, d_ff=d_ff, dw_ks=dw_ks, norm=norm, dropout=dropout, act=act,head_dropout=head_dropout, padding_patch = padding_patch, head_type=head_type, 
                            revin=revin, affine=affine, deformable=True, subtract_last=subtract_last, enable_res_param=enable_res_param, re_param=re_param, re_param_kernel=re_param_kernel).to(device)
    
model.eval()  # ç¡®ä¿åœ¨æµ‹è¯•æ¨¡å¼
# --------------------------------------------
# æµ‹è¯•1ï¼šéªŒè¯RevINå½’ä¸€åŒ–æ˜¯å¦ä½¿ç”¨æœªæ¥ä¿¡æ¯
# --------------------------------------------
def test_revin():
    # æ„é€ æµ‹è¯•æ•°æ®ï¼šå‰åŠéƒ¨åˆ†å…¨0ï¼ŒååŠéƒ¨åˆ†å…¨1
    test_input = torch.cat([
        torch.zeros(1, c_in, context_window//2),
        torch.ones(1, c_in, context_window//2)
    ], dim=-1).to(device)
    
    # å‰å‘è¿‡ç¨‹
    with torch.no_grad():
        output = model(test_input)
    # print(f"output: {output}")
    # æ£€æŸ¥åå½’ä¸€åŒ–åçš„è¾“å‡ºèŒƒå›´
    if output.max() <= 1.0 and output.min() >= 0.0:
        print("RevINæµ‹è¯•é€šè¿‡ï¼šè¾“å‡ºèŒƒå›´æ­£å¸¸")
    else:
        print("RevINè­¦å‘Šï¼šæ£€æµ‹åˆ°éå¸¸è§„è¾“å‡ºèŒƒå›´")

# --------------------------------------------
# æµ‹è¯•2ï¼šéªŒè¯åˆ†å—é‡‡æ ·ä¸è®¿é—®æœªæ¥ä¿¡æ¯
# --------------------------------------------
def test_sampling():
    patch_num = int((context_window - patch_len)/stride + 1)
    # æ„é€ å…·æœ‰æ˜æ˜¾æ—¶é—´ç‰¹å¾çš„æµ‹è¯•æ•°æ®
    time_series = torch.arange(context_window).float().view(1, 1, context_window).to(device)
    time_series = time_series.repeat(1, c_in, 1).to(device) 
    if padding_patch == 'end': # can be modified to general case
        # self.padding_patch_layer = nn.ReplicationPad1d((0, stride*2)) 
        padding_patch_layer = nn.ReplicationPad1d(( stride,0)) 
        time_series = padding_patch_layer(time_series)
        patch_num += 1
    
    # è·å–é‡‡æ ·ä½ç½®
    with torch.no_grad():
        sampling_locations = model.deformable_sampling.get_sampling_location(time_series)[0]
    
    # éªŒè¯é‡‡æ ·ä½ç½®ä¸è¶…è¿‡å½“å‰çª—å£
    max_position = (sampling_locations[..., 0] * (context_window-1)).max().item()
    if max_position < context_window-1:
        print("é‡‡æ ·æµ‹è¯•é€šè¿‡ï¼šæœªæ£€æµ‹åˆ°æœªæ¥ä½ç½®é‡‡æ ·")
    else:
        print(f"é‡‡æ ·è­¦å‘Šï¼šæ£€æµ‹åˆ°æœªæ¥ä½ç½®é‡‡æ ·ï¼ˆæœ€å¤§ä½ç½® {max_position:.1f}ï¼‰")

# --------------------------------------------
# æµ‹è¯•3ï¼šéªŒè¯å› æœå·ç§¯çš„æœ‰æ•ˆæ€§
# --------------------------------------------
def test_causal_conv():
    # æ„é€ è„‰å†²è¾“å…¥æµ‹è¯•æ•°æ®
    test_input = torch.zeros(1, c_in, context_window).to(device)
    test_input[0, 0, context_window//2] = 1.0  # ä¸­é—´ä½ç½®æ”¾ç½®è„‰å†²
    
    # æ£€æŸ¥æ‰€æœ‰å·ç§¯å±‚
    for name, layer in model.named_modules():
        if isinstance(layer, nn.Conv1d):
            # éªŒè¯å·ç§¯çš„paddingæ–¹å¼
            
            if layer.padding[0] != (layer.kernel_size[0]-1):
                print(f"layer.padding[0]: {layer.padding[0]}")
                print(f"å·ç§¯å±‚ '{name}' è­¦å‘Šï¼šæ£€æµ‹åˆ°éå› æœå¡«å……æ–¹å¼")
    
    # å‰å‘ä¼ æ’­éªŒè¯
    with torch.no_grad():
        output = model(test_input)
    
    # æ£€æŸ¥å“åº”ä½ç½®
    response_pos = output.argmax()
    if response_pos >= context_window//2:
        print("å› æœå·ç§¯æµ‹è¯•é€šè¿‡ï¼šå“åº”ä½ç½®æ­£ç¡®")
    else:
        print(f"å› æœå·ç§¯è­¦å‘Šï¼šæ£€æµ‹åˆ°æå‰å“åº”ï¼ˆä½ç½® {response_pos.item()}ï¼‰")

# --------------------------------------------
# æµ‹è¯•4ï¼šå®Œæ•´å‰å‘ä¼ æ’­æ³„éœ²æµ‹è¯•
# --------------------------------------------
def test_full_forward():
    # æ„é€ æµ‹è¯•æ•°æ®ï¼šå‰N-1ä¸ªæ—¶é—´æ­¥ä¸ºéšæœºå™ªå£°ï¼Œæœ€å1æ­¥ä¸ºç‰¹æ®Šå€¼
    normal_data = torch.randn(1, c_in, context_window-1)
    test_input = torch.cat([
        normal_data,
        torch.full((1, c_in, 1), 10.0)
    ], dim=-1).to(device)
    
    # å‰å‘è¿‡ç¨‹
    with torch.no_grad():
        output = model(test_input)
    
    # åˆ†æè¾“å‡ºç‰¹å¾
    output_window = output[0, 0, :].cpu().numpy()
    anomaly_ratio = (output_window > 8.0).mean()
    
    if anomaly_ratio < 0.2:
        print("å®Œæ•´å‰å‘æµ‹è¯•é€šè¿‡ï¼šæœªæ£€æµ‹åˆ°å¼‚å¸¸ä¼ æ’­")
    else:
        print(f"æ³„éœ²è­¦å‘Šï¼šæ£€æµ‹åˆ°å¼‚å¸¸ä¼ æ’­æ¯”ä¾‹ {anomaly_ratio*100:.1f}%")

# æ‰§è¡Œæ‰€æœ‰æµ‹è¯•
if __name__ == "__main__":
    print("å¼€å§‹æ³„éœ²æµ‹è¯•...")
    test_revin()
    test_sampling()
    test_causal_conv()
    test_full_forward()
    print("æµ‹è¯•å®Œæˆ")
# def test_padding_operation():
    
    

#     # æ„é€ è¾“å…¥æ•°æ®ï¼ˆå‡è®¾å½“å‰æ—¶é—´ç‚¹ä¸ºt=context_windowï¼‰
#     X = torch.randn(1, c_in, context_window) .to(device)
    
#     # å‰å‘ä¼ æ’­
#     output = model(X)
    
#     # æ£€æŸ¥å¡«å……åçš„é•¿åº¦æ˜¯å¦ç¬¦åˆé¢„æœŸ
#     padded_length = context_window + padding_stride
#     print(f"model.padding_patch_layer.padding: {model.padding_patch_layer.padding}")
#     assert model.padding_patch_layer.padding == (padding_stride, 0), "å¡«å……æ–¹å‘é”™è¯¯ï¼åº”ä¸ºå·¦ä¾§å¡«å……"
    
    

#     # éªŒè¯è¡¥ä¸ç”Ÿæˆæ˜¯å¦ä»…ä½¿ç”¨å†å²æ•°æ®
#     unfold = X.unfold(dimension=-1, size=patch_len, step=stride)
#     assert unfold.shape[-1] == patch_len, "è¡¥ä¸é•¿åº¦é”™è¯¯"
#     print("æµ‹è¯•é€šè¿‡ï¼šå¡«å……æ“ä½œæœªæ³„éœ²æœªæ¥ä¿¡æ¯")

# test_padding_operation()

# def test_deformable_patch_boundaries():


#     # åˆ›å»ºå¯å˜å½¢è¡¥ä¸æ¨¡å—
#     depatch = DepatchSampling(c_in, seq_len, patch_len, stride).to(device)

#     # æ„é€ è¾“å…¥æ•°æ®ï¼ˆå½“å‰æ—¶é—´ç‚¹ä¸ºt=seq_lenï¼‰
#     X = torch.randn(2, c_in, seq_len).to(device)  # [B=2, C=1, L=100]

#     # è·å–é‡‡æ ·ä½ç½®
#     sampling_locations, bound = depatch.get_sampling_location(X)

#     # å°†å½’ä¸€åŒ–åæ ‡è½¬æ¢ä¸ºå®é™…ç´¢å¼•
#     bound_indices = bound * (seq_len - 1)
#     right_bound = bound_indices[..., 1]  # å³è¾¹ç•Œç´¢å¼•

#     # æ£€æŸ¥æ‰€æœ‰å³è¾¹ç•Œæ˜¯å¦ä¸è¶…è¿‡å½“å‰æ—¶é—´ç‚¹
#     violation = (right_bound > seq_len - 1).any()
#     assert not violation, "å¯å˜å½¢è¡¥ä¸å³è¾¹ç•Œæ³„éœ²æœªæ¥ä¿¡æ¯ï¼"
#     print("æµ‹è¯•é€šè¿‡ï¼šå¯å˜å½¢è¡¥ä¸è¾¹ç•Œæœªè¶Šç•Œ")

# test_deformable_patch_boundaries()

# def test_revin_normalization():
#     # éªŒè¯RevINæ˜¯å¦ç‹¬ç«‹å¤„ç†æ¯ä¸ªåºåˆ—

#     model = RevIN(c_in, affine=affine, subtract_last=subtract_last)

#     # æ„é€ ä¸¤ä¸ªä¸åŒå‡å€¼çš„åºåˆ—
#     X1 = torch.randn(1, seq_len, c_in) * 10 + 5  # å‡å€¼~5
#     X2 = torch.randn(1, seq_len, c_in) * 10 - 5  # å‡å€¼~-5

#     # å½’ä¸€åŒ–ä¸åå½’ä¸€åŒ–
#     norm_X1 = model(X1, 'norm')
#     denorm_X1 = model(norm_X1, 'denorm')

#     norm_X2 = model(X2, 'norm')
#     denorm_X2 = model(norm_X2, 'denorm')

#     # æ£€æŸ¥æ˜¯å¦æ¢å¤åŸå§‹æ•°æ®
#     assert torch.allclose(X1, denorm_X1, atol=1e-4), "RevINåå½’ä¸€åŒ–å¤±è´¥"
#     assert torch.allclose(X2, denorm_X2, atol=1e-4), "RevINåå½’ä¸€åŒ–å¤±è´¥"

#     # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨å…¨å±€ç»Ÿè®¡é‡ï¼ˆåº”å¤±è´¥ï¼‰
#     global_mean = X1.mean(dim=1, keepdim=True)
#     local_mean = model.mean
#     assert not torch.allclose(global_mean, local_mean), "RevINä½¿ç”¨äº†å…¨å±€ç»Ÿè®¡é‡ï¼"
#     print("æµ‹è¯•é€šè¿‡ï¼šRevINæ— ä¿¡æ¯æ³„éœ²")

# test_revin_normalization()

# def test_unfold_operation():


#     # æ„é€ è¾“å…¥æ•°æ®ï¼ˆå½“å‰æ—¶é—´ç‚¹ä¸ºt=100ï¼‰
#     X = torch.randn(1, c_in, context_window)  # [B=1, C=1, L=100]

#     # ç”Ÿæˆè¡¥ä¸
#     # patches = X.unfold(dimension=-1, size=patch_len, step=stride)
#     X = X.unsqueeze(1).permute(0, 1, 3, 2)
#     patches = F.unfold(X, kernel_size=(patch_len, c_in), stride=stride)
#     # æ£€æŸ¥æœ€åä¸€ä¸ªè¡¥ä¸çš„å³è¾¹ç•Œ
#     last_patch_end = (patches.shape[-1] - 1) * stride + patch_len
#     print(f"last_patch_end: {last_patch_end}")
#     print(f"context_window: {context_window}")
#     assert last_patch_end <= context_window, "è¡¥ä¸å±•å¼€æ³„éœ²æœªæ¥ä¿¡æ¯ï¼"
#     print("æµ‹è¯•é€šè¿‡ï¼šæ™®é€šè¡¥ä¸å±•å¼€æœªè¶Šç•Œ")

# test_unfold_operation()

# ########################################
# # æ£€æµ‹å·¥å…·å‡½æ•°
# ########################################
# def check_temporal_leakage(model, seq_len=5, verbose=True):
#     """æµ‹è¯•æ—¶é—´çª—å£æ˜¯å¦åŒ…å«æœªæ¥ä¿¡æ¯"""
#     # ç”Ÿæˆæµ‹è¯•æ•°æ®
#     X = torch.arange(seq_len).float().view(1, 1, seq_len).repeat(1, model.channel, 1)
    
#     # è®¡ç®—unfoldå‚æ•°
#     K = model.patch_size
#     S = model.stride
#     num_patches = (seq_len - K) // S + 1
    
#     # åˆ†ææ¯ä¸ªpatchçš„æ—¶é—´èŒƒå›´
#     leakages = []
#     for i in range(num_patches):
#         start = i * S
#         end = start + K - 1
#         if end >= seq_len - S:  # æœ€åä¸€ä¸ªpatchçš„ç»“æŸä½ç½®è¶…è¿‡å½“å‰å…è®¸èŒƒå›´
#             leakages.append((i, start, end))
    
#     if verbose:
#         print(f"Time window analysis (seq_len={seq_len}, patch_size={K}, stride={S}):")
#         for i, s, e in leakages:
#             print(f"  Patch {i}: [{s}-{e}] contains future timesteps")
    
#     return len(leakages) > 0

# def causality_test(model, inject_position=-1):
#     """é€šè¿‡æ³¨å…¥ç‰¹æ®Šå€¼æ£€æµ‹å› æœå…³ç³»"""
#     # ç”Ÿæˆæµ‹è¯•æ•°æ®
#     seq_len = 5
#     X = torch.randn(1, model.channel, seq_len)
    
#     # æ³¨å…¥ç‰¹æ®Šå€¼ï¼ˆå¦‚NaNï¼‰
#     original_value = X[0, 0, inject_position].clone()
#     X[0, 0, inject_position] = float('nan')
    
#     try:
#         with torch.no_grad():
#             output = model(X)
#             affected = torch.isnan(output).any()
#             return not affected  # å¦‚æœè¾“å‡ºä¸å—å½±å“ï¼Œè¯´æ˜æ²¡æœ‰ä½¿ç”¨æœªæ¥ä¿¡æ¯
#     except Exception as e:
#         print(f"Error during inference: {str(e)}")
#         return False

# def autoregressive_simulation(model, steps=3):
#     """æ¨¡æ‹Ÿè‡ªå›å½’æ¨ç†è¿‡ç¨‹"""
#     history = []
#     current_seq = torch.randn(1, model.channel, seq_len)  # åˆå§‹åªæœ‰ä¸€ä¸ªæ—¶é—´æ­¥
    
#     for t in range(steps):
#         try:
#             # æ‰©å±•åºåˆ—é•¿åº¦

            
#             # å°è¯•ç”Ÿæˆoffset
#             with torch.no_grad():
#                 _ = model(current_seq)
                
#         except RuntimeError as e:
#             if "unfold" in str(e) and "out of range" in str(e):
#                 print(f"At step {t}: Failed to process due to future data requirement")
#                 return True  # æ£€æµ‹åˆ°éœ€è¦æœªæ¥æ•°æ®
#             raise
#     return False

# ########################################
# # ç»¼åˆæµ‹è¯•
# ########################################
# def run_full_diagnosis(in_feats=1, patch_size=3, stride=1):
#     print("="*60)
#     print(f"Running diagnostics for OffsetPredictor (patch_size={patch_size}, stride={stride})")
#     print("="*60)
    
#     model = OffsetPredictor(in_feats, patch_size, stride)
    
#     # æµ‹è¯•1: æ—¶é—´çª—å£åˆ†æ
#     has_leakage = check_temporal_leakage(model, seq_len=5)
#     print(f"\nTest 1 - Temporal Window Analysis: {'Leakage detected' if has_leakage else 'No leakage'}")
    
#     # æµ‹è¯•2: å› æœæ€§æµ‹è¯•ï¼ˆæ³¨å…¥ç‰¹æ®Šå€¼ï¼‰
#     causal_safe = causality_test(model)
#     print(f"Test 2 - Causality Test: {'Safe' if causal_safe else 'Leakage detected'}")
    
#     # æµ‹è¯•3: è‡ªå›å½’æ¨¡æ‹Ÿ
#     requires_future = autoregressive_simulation(model)
#     print(f"Test 3 - Autoregressive Simulation: {'Valid' if not requires_future else 'Invalid (needs future data)'}")
    
#     # ç»¼åˆç»“è®º
#     if any([has_leakage, not causal_safe, requires_future]):
#         print("\nğŸš¨ Conclusion: MODERN LEAKAGE DETECTED!")
#     else:
#         print("\nâœ… Conclusion: No leakage detected")

# ########################################
# # è¿è¡Œæµ‹è¯•
# ########################################
# if __name__ == "__main__":
#     # æµ‹è¯•ä¸åŒé…ç½®
#     run_full_diagnosis(patch_size=patch_len, stride=stride)  # æ˜æ˜¾ä¼šæ³„éœ²çš„é…ç½®
#     run_full_diagnosis(patch_size=patch_len, stride=stride)  # å®‰å…¨é…ç½®ï¼ˆstrideç­‰äºpatch_sizeï¼‰
